# HTTP API port exposed by helper scripts and Docker Compose.
API_PORT=18080

# PostgreSQL host port used by local metadata database.
PG_PORT=56473

# Qdrant host port used by local vector database.
QDRANT_PORT=6333

# SQLAlchemy connection string for metadata DB (api keys, task state, schema meta).
DATABASE_URL=postgresql://rag:rag@localhost:56473/rag

# Base URL of the Qdrant instance used for vector storage.
QDRANT_URL=http://localhost:6333

# Optional API key for authenticated Qdrant deployments.
QDRANT_API_KEY=

# Qdrant collection name where document segments are stored.
QDRANT_COLLECTION=rag_segments

# Optional absolute repo root override for path-sensitive extraction helpers.
RAG_REPO_ROOT=

# Base URL for OpenAI-compatible inference server (chat + embeddings default).
INFERENCE_BASE_URL=

# Optional API key for the default inference provider.
INFERENCE_API_KEY=

# Default chat model used when request model is not provided.
INFERENCE_CHAT_MODEL=local-model

# Default embeddings model used when provider-specific override is not set.
INFERENCE_EMBEDDING_MODEL=BAAI/bge-m3

# Chat backend adapter: `openai_compat` or `litellm`.
CHAT_BACKEND=openai_compat

# Optional chat provider base URL override.
CHAT_BASE_URL=

# Optional API key for chat provider override.
CHAT_API_KEY=

# Optional explicit chat model (provider-prefixed for LiteLLM when needed).
CHAT_MODEL=

# Optional Vertex project for chat when using LiteLLM Vertex integration.
CHAT_VERTEX_PROJECT=

# Optional Vertex region for chat requests.
CHAT_VERTEX_LOCATION=us-central1

# Optional path to Vertex service account credentials for chat.
CHAT_VERTEX_CREDENTIALS=

# Optional generic Vertex project fallback used by chat/embeddings settings loader.
VERTEX_PROJECT=

# Optional generic Vertex location fallback used by settings loader.
VERTEX_LOCATION=

# Optional generic Vertex credentials fallback used by settings loader.
VERTEX_CREDENTIALS=

# Optional Google Cloud project fallback (used for Vertex defaults).
GOOGLE_CLOUD_PROJECT=

# Optional ADC/service account path fallback (used for Vertex defaults).
GOOGLE_APPLICATION_CREDENTIALS=

# Embeddings backend adapter: `openai_compat` or `litellm`.
EMBEDDINGS_BACKEND=openai_compat

# Optional embeddings provider base URL override.
EMBEDDINGS_BASE_URL=

# Optional API key for embeddings provider override.
EMBEDDINGS_API_KEY=

# Embeddings model used for ingest and retrieval query embedding.
EMBEDDINGS_MODEL=BAAI/bge-m3

# Optional Vertex project for embeddings when using LiteLLM Vertex integration.
EMBEDDINGS_VERTEX_PROJECT=

# Optional Vertex region for embeddings requests.
EMBEDDINGS_VERTEX_LOCATION=us-central1

# Optional path to Vertex service account credentials for embeddings.
EMBEDDINGS_VERTEX_CREDENTIALS=

# Embedding vector dimension; keep empty to auto-probe from embeddings model.
EMBEDDING_DIM=1024

# Legacy LM Studio base URL fallback (kept for backward compatibility).
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Legacy LM Studio API key fallback.
LMSTUDIO_API_KEY=

# Legacy LM Studio chat model fallback.
LMSTUDIO_CHAT_MODEL=local-model

# Legacy LM Studio embeddings model fallback.
LMSTUDIO_EMBEDDING_MODEL=local-embedding-model

# Number of retrieved segments sent to context builder.
TOP_K=6

# Maximum total context characters assembled before LLM call.
MAX_CONTEXT_CHARS=24000

# Enable hybrid retrieval (vector + lexical RRF) when set to 1.
RETRIEVAL_USE_FTS=1

# Enforce model availability checks during API startup / ingest preflight.
STRICT_MODEL_STARTUP=0

# Try to auto download+load missing local models via `lms get/load`.
LMSTUDIO_AUTO_PULL=0

# Chunking strategy for PDF ingestion pipeline.
CHUNKING_STRATEGY=semantic

# Base chunk size used by chunking strategies that support fixed size.
CHUNKING_CHUNK_SIZE=512

# Character overlap between adjacent chunks (mainly sliding strategy).
CHUNKING_OVERLAP_CHARS=200

# Similarity threshold used by semantic chunking strategy.
CHUNKING_SIMILARITY_THRESHOLD=0.5

# Enable post-chunk sanitization pass.
CHUNK_SANITIZE_ENABLED=1

# Minimum word count for kept chunks during sanitization.
CHUNK_SANITIZE_MIN_WORDS=3

# Enable duplicate chunk removal in sanitization.
CHUNK_SANITIZE_DEDUP=1

# PDF text extractor implementation (`docling` is currently supported).
PDF_TEXT_EXTRACTOR=docling

# Enable OCR in Docling extraction mode.
DOCLING_DO_OCR=1

# Enable Docling table structure extraction.
DOCLING_DO_TABLE_STRUCTURE=0

# Force full-page OCR in Docling OCR options.
DOCLING_FORCE_FULL_PAGE_OCR=0

# Force backend text extraction path in Docling.
DOCLING_FORCE_BACKEND_TEXT=0

# Include picture elements in Docling export.
DOCLING_INCLUDE_PICTURES=0

# Enable picture classification in Docling pipeline.
DOCLING_DO_PICTURE_CLASSIFICATION=0

# Enable picture caption/description generation in Docling pipeline.
DOCLING_DO_PICTURE_DESCRIPTION=0

# Auto-toggle OCR based on detected text layer quality.
DOCLING_OCR_AUTO=1

# Text-layer ratio threshold above which OCR is auto-disabled.
DOCLING_OCR_AUTO_TEXT_LAYER_THRESHOLD=0.9

# Minimum extracted chars per sampled page to count as text-layer page.
DOCLING_OCR_AUTO_MIN_CHARS=20

# Number of pages to sample for OCR auto-detection (0 means all pages).
DOCLING_OCR_AUTO_SAMPLE_PAGES=0

# Enable markdown dump of extracted PDF pages when truthy.
PDF_DUMP_MD=

# Output directory for extracted markdown dumps.
PDF_DUMP_DIR=var/extracted

# Heartbeat interval in seconds for `scripts/pdf_to_md_local.sh`.
PDF_LOG_HEARTBEAT_SEC=15

# Internal helper input path used by PDF conversion scripts.
INPUT_PDF_PATH=

# Ingest wrapper mode: `extract`, `ingest`, or `resume`.
INGEST_MODE=ingest

# Error policy for ingest run: `fail` or `skip`.
INGEST_ON_ERROR=fail

# Existing ingest task ID used when `INGEST_MODE=resume`.
INGEST_TASK_ID=

# Force reprocessing even when hashes/state indicate up-to-date.
INGEST_FORCE=

# Source PDF directory used by extract mode.
INGEST_PDF_DIR=var/pdfs

# Source chunks directory used by ingest mode.
INGEST_CHUNKS_DIR=var/extracted

# Output directory for extract-only mode chunk files.
INGEST_EXTRACT_OUTPUT_DIR=var/extracted

# Python interpreter path for local host scripts.
PYTHON_BIN=.venv/bin/python

# Embeddings batch size during ingest indexing.
INGEST_EMBED_BATCH_SIZE=128

# Allow unauthenticated API access when set to true.
ALLOW_ANONYMOUS=false

# Optional explicit API key passed to create-api-key helper script.
API_KEY=

# API key tier assigned by create-api-key helper (`pro`, etc.).
TIER=pro

# Whether generated API key has citations entitlement enabled.
CITATIONS_ENABLED=false

# Enable request prompt logging when truthy.
LOG_PROMPTS=

# Enable completion text/stream logging when truthy.
LOG_COMPLETIONS=

# Application log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
LOG_LEVEL=INFO

# Log output format: `pretty` or `json`.
LOG_FORMAT=pretty
