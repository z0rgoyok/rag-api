# Ports used by scripts/compose
API_PORT=18080
PG_PORT=56473
QDRANT_PORT=6333

# Metadata DB (auth keys + ingest task state)
DATABASE_URL=postgresql://rag:rag@localhost:56473/rag

# Vector DB (semantic index)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=rag_segments

# Repo root override for path resolution in extraction helpers
RAG_REPO_ROOT=

# Inference provider (OpenAI-compatible). Works with LM Studio or external providers.
# Local (non-docker LM Studio): http://localhost:1234/v1
# Docker Desktop (compose -> host LM Studio): http://host.docker.internal:1234/v1
# Keep empty for auto:
# - host scripts -> fallback to http://localhost:1234/v1
# - docker compose api -> fallback to LMSTUDIO_BASE_URL (host.docker.internal by default in compose)
INFERENCE_BASE_URL=
INFERENCE_API_KEY=
INFERENCE_CHAT_MODEL=local-model
INFERENCE_EMBEDDING_MODEL=local-embedding-model

# Optional split providers (override INFERENCE_* for chat)
CHAT_BACKEND=openai_compat
CHAT_BASE_URL=
CHAT_API_KEY=
CHAT_MODEL=
CHAT_VERTEX_PROJECT=
CHAT_VERTEX_LOCATION=us-central1
CHAT_VERTEX_CREDENTIALS=

# Optional global Vertex/GCP fallbacks for CHAT_* / EMBEDDINGS_*
VERTEX_PROJECT=
VERTEX_LOCATION=
VERTEX_CREDENTIALS=
GOOGLE_CLOUD_PROJECT=
GOOGLE_APPLICATION_CREDENTIALS=

# Embeddings provider
# - openai_compat: direct OpenAI-compatible HTTP calls (default)
# - litellm: route via LiteLLM (supports many providers)
EMBEDDINGS_BACKEND=openai_compat
EMBEDDINGS_BASE_URL=
EMBEDDINGS_API_KEY=
EMBEDDINGS_MODEL=
EMBEDDINGS_VERTEX_PROJECT=
EMBEDDINGS_VERTEX_LOCATION=us-central1
EMBEDDINGS_VERTEX_CREDENTIALS=

# Optional explicit embedding dimension. If empty, probed from embedding model.
EMBEDDING_DIM=

# Legacy (still supported): LMSTUDIO_*
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_API_KEY=
LMSTUDIO_CHAT_MODEL=local-model
LMSTUDIO_EMBEDDING_MODEL=local-embedding-model

# RAG behavior
TOP_K=6
MAX_CONTEXT_CHARS=24000
RETRIEVAL_USE_FTS=1          # set 0 for pure vector retrieval

# Chunking strategy for ingestion
# - sliding: fixed-size window with overlap (fast, may cut mid-sentence)
# - recursive: hierarchical split by paragraphs/sentences/words (good balance)
# - semantic: split where embedding similarity drops (best for books, requires chonkie)
# - docling_hierarchical: use docling-core structural chunker (host/local extract only)
# - docling_hybrid: use docling-core hybrid chunker (host/local extract only)
CHUNKING_STRATEGY=semantic
CHUNKING_CHUNK_SIZE=512
CHUNKING_OVERLAP_CHARS=200
CHUNKING_SIMILARITY_THRESHOLD=0.5
CHUNK_SANITIZE_ENABLED=1
CHUNK_SANITIZE_MIN_WORDS=3
CHUNK_SANITIZE_DEDUP=1

# PDF extraction options (docling)
PDF_TEXT_EXTRACTOR=docling
DOCLING_DO_OCR=1
DOCLING_DO_TABLE_STRUCTURE=0
DOCLING_FORCE_FULL_PAGE_OCR=0
DOCLING_FORCE_BACKEND_TEXT=0
DOCLING_INCLUDE_PICTURES=0
DOCLING_DO_PICTURE_CLASSIFICATION=0
DOCLING_DO_PICTURE_DESCRIPTION=0
DOCLING_OCR_AUTO=1
DOCLING_OCR_AUTO_TEXT_LAYER_THRESHOLD=0.9
DOCLING_OCR_AUTO_MIN_CHARS=20
DOCLING_OCR_AUTO_SAMPLE_PAGES=0
PDF_DUMP_MD=
PDF_DUMP_DIR=var/extracted

# scripts/pdf_to_md_local.sh heartbeat interval (seconds)
PDF_LOG_HEARTBEAT_SEC=15

# Internal helper input for pdf_to_md scripts (normally set by script itself)
INPUT_PDF_PATH=

# Ingestion wrappers
# scripts/ingest.sh: extract | ingest | resume
INGEST_MODE=ingest
INGEST_ON_ERROR=fail
INGEST_TASK_ID=
INGEST_FORCE=
INGEST_PDF_DIR=var/pdfs
INGEST_CHUNKS_DIR=var/extracted
INGEST_EXTRACT_OUTPUT_DIR=var/extracted
PYTHON_BIN=.venv/bin/python
INGEST_EMBED_BATCH_SIZE=128

# Security (MVP)
# Create keys via `python -m apps.api.scripts.create_api_key`
ALLOW_ANONYMOUS=false

# Helper inputs for create-api-key script
API_KEY=
TIER=pro
CITATIONS_ENABLED=false

# Debug logs (never prints secrets)
LOG_PROMPTS=
LOG_COMPLETIONS=
LOG_LEVEL=INFO
LOG_FORMAT=pretty
