# HTTP API port exposed by helper scripts and Docker Compose.
API_PORT=18080

# NextChat UI host port.
NEXTCHAT_PORT=3000

# NextChat OpenAI-compatible base URL.
# Important: NextChat appends `/v1` itself, so keep this without `/v1`.
NEXTCHAT_BASE_URL=http://api:8080

# API key used by NextChat for calls to rag-api (`/v1/chat/completions`).
# When `ALLOW_ANONYMOUS=false`, generate via `./scripts/create-api-key.sh`.
# NEXTCHAT_OPENAI_API_KEY=

# Optional comma-separated model ids shown in NextChat model selector.
# Example: NEXTCHAT_CUSTOM_MODELS=vertex_ai/gemini-3-flash-preview,local-model
# NEXTCHAT_CUSTOM_MODELS=

# Optional default model from NEXTCHAT_CUSTOM_MODELS.
# NEXTCHAT_DEFAULT_MODEL=

# Optional NextChat access code (UI passcode). Keep empty for local-only test.
# NEXTCHAT_CODE=

# Enable Realtime Chat toggle by default in NextChat settings.
# Applied at container startup by patching NextChat bundled defaults.
NEXTCHAT_ENABLE_REALTIME_DEFAULT=1

# PostgreSQL host port used by local metadata database.
PG_PORT=56473

# Qdrant host port used by local vector database.
QDRANT_PORT=6333

# SQLAlchemy connection string for metadata DB (api keys, task state, schema meta).
DATABASE_URL=postgresql://rag:rag@localhost:56473/rag

# Base URL of the Qdrant instance used for vector storage.
QDRANT_URL=http://localhost:6333

# Optional API key for authenticated Qdrant deployments.
# QDRANT_API_KEY=

# Qdrant collection name where document segments are stored.
QDRANT_COLLECTION=rag_segments

# Optional absolute repo root override for path-sensitive extraction helpers.
# RAG_REPO_ROOT=

# Base URL for OpenAI-compatible inference server (chat + embeddings default).
# INFERENCE_BASE_URL=

# Optional API key for the default inference provider.
# INFERENCE_API_KEY=

# Default chat model used when request model is not provided.
INFERENCE_CHAT_MODEL=local-model

# Default embeddings model used when provider-specific override is not set.
INFERENCE_EMBEDDING_MODEL=BAAI/bge-m3

# Chat backend adapter: `openai_compat` or `litellm`.
CHAT_BACKEND=litellm

# Optional chat provider base URL override.
# CHAT_BASE_URL=

# Optional API key for chat provider override.
# CHAT_API_KEY=

# Optional explicit chat model (provider-prefixed for LiteLLM when needed).
CHAT_MODEL=vertex_ai/gemini-3-flash-preview

# Optional Vertex project for chat when using LiteLLM Vertex integration.
CHAT_VERTEX_PROJECT=trends-479621

# Optional Vertex region for chat requests.
CHAT_VERTEX_LOCATION=global

# Optional path to Vertex service account credentials for chat.
CHAT_VERTEX_CREDENTIALS=/app/var/gcp/service-account.json

# Optional generic Vertex project fallback used by chat/embeddings settings loader.
# VERTEX_PROJECT=

# Optional generic Vertex location fallback used by settings loader.
# VERTEX_LOCATION=

# Optional generic Vertex credentials fallback used by settings loader.
# VERTEX_CREDENTIALS=

# Optional Google Cloud project fallback (used for Vertex defaults).
# GOOGLE_CLOUD_PROJECT=

# Optional ADC/service account path fallback (used for Vertex defaults).
# GOOGLE_APPLICATION_CREDENTIALS=

# Embeddings backend adapter: `openai_compat` or `litellm`.
EMBEDDINGS_BACKEND=openai_compat

# Optional embeddings provider base URL override.
# EMBEDDINGS_BASE_URL=

# Optional API key for embeddings provider override.
# EMBEDDINGS_API_KEY=

# Embeddings model used for ingest and retrieval query embedding.
EMBEDDINGS_MODEL=BAAI/bge-m3

# Optional Vertex project for embeddings when using LiteLLM Vertex integration.
# EMBEDDINGS_VERTEX_PROJECT=

# Optional Vertex region for embeddings requests.
EMBEDDINGS_VERTEX_LOCATION=us-central1

# Optional path to Vertex service account credentials for embeddings.
# EMBEDDINGS_VERTEX_CREDENTIALS=

# Embedding vector dimension; keep empty to auto-probe from embeddings model.
EMBEDDING_DIM=1024

# Legacy LM Studio base URL fallback (kept for backward compatibility).
# LMSTUDIO_BASE_URL=

# Legacy LM Studio API key fallback.
# LMSTUDIO_API_KEY=

# Legacy LM Studio chat model fallback.
# LMSTUDIO_CHAT_MODEL=

# Legacy LM Studio embeddings model fallback.
# LMSTUDIO_EMBEDDING_MODEL=

# Number of retrieved segments sent to context builder.
TOP_K=6

# Maximum total context characters assembled before LLM call.
MAX_CONTEXT_CHARS=24000

# Enable hybrid retrieval (vector + lexical RRF) when set to 1.
RETRIEVAL_USE_FTS=1

# Reranking strategy: `none`, `lmstudio`, `cross_encoder`, `cohere`, `http`.
RERANKING_STRATEGY=http

# How many retrieval candidates to fetch before reranking.
RERANKING_RETRIEVAL_K=50

# Optional reranker provider base URL:
# - `lmstudio`: LM Studio OpenAI-compatible base URL (e.g. `http://host.docker.internal:1234/v1`)
# - `http`: host reranker service base URL (e.g. `http://host.docker.internal:18123`)
RERANKING_BASE_URL=http://host.docker.internal:18123

# Optional reranker provider API key (falls back to INFERENCE_API_KEY).
# RERANKING_API_KEY=

# Reranker model id:
# - `cross_encoder`: Hugging Face model id (e.g. `BAAI/bge-reranker-v2-m3`)
# - `lmstudio`: model id exposed by LM Studio `/v1/models`
RERANKING_MODEL=BAAI/bge-reranker-v2-m3

# Batch size for reranker requests.
RERANKING_BATCH_SIZE=64

# Host reranker service (`scripts/rerank-host.sh`) settings.
RERANK_HOST_BIND=0.0.0.0
RERANK_HOST_PORT=18123
RERANK_HOST_DEFAULT_MODEL=BAAI/bge-reranker-v2-m3
RERANK_HOST_BATCH_SIZE=64
# RERANK_HOST_API_KEY=

# Chunking strategy for PDF ingestion pipeline.
CHUNKING_STRATEGY=semantic

# Base chunk size used by chunking strategies that support fixed size.
CHUNKING_CHUNK_SIZE=1024

# Character overlap between adjacent chunks (mainly sliding strategy).
CHUNKING_OVERLAP_CHARS=200

# Similarity threshold used by semantic chunking strategy.
CHUNKING_SIMILARITY_THRESHOLD=0.5

# Enable post-chunk sanitization pass.
CHUNK_SANITIZE_ENABLED=1

# Minimum word count for kept chunks during sanitization.
CHUNK_SANITIZE_MIN_WORDS=3

# Enable duplicate chunk removal in sanitization.
CHUNK_SANITIZE_DEDUP=1

# PDF text extractor implementation (`docling` is currently supported).
PDF_TEXT_EXTRACTOR=docling

# Enable OCR in Docling extraction mode.
DOCLING_DO_OCR=0

# Enable Docling table structure extraction.
DOCLING_DO_TABLE_STRUCTURE=0

# Force full-page OCR in Docling OCR options.
DOCLING_FORCE_FULL_PAGE_OCR=0

# Force backend text extraction path in Docling.
DOCLING_FORCE_BACKEND_TEXT=0

# Include picture elements in Docling export.
DOCLING_INCLUDE_PICTURES=0

# Enable picture classification in Docling pipeline.
DOCLING_DO_PICTURE_CLASSIFICATION=0

# Enable picture caption/description generation in Docling pipeline.
DOCLING_DO_PICTURE_DESCRIPTION=0

# Auto-toggle OCR based on detected text layer quality.
DOCLING_OCR_AUTO=1

# Text-layer ratio threshold above which OCR is auto-disabled.
DOCLING_OCR_AUTO_TEXT_LAYER_THRESHOLD=0.9

# Minimum extracted chars per sampled page to count as text-layer page.
DOCLING_OCR_AUTO_MIN_CHARS=20

# Number of pages to sample for OCR auto-detection (0 means all pages).
DOCLING_OCR_AUTO_SAMPLE_PAGES=0

# Enable markdown dump of extracted PDF pages when truthy.
# PDF_DUMP_MD=

# Output directory for extracted markdown dumps.
PDF_DUMP_DIR=var/extracted

# Heartbeat interval in seconds for `scripts/pdf_to_md_local.sh`.
PDF_LOG_HEARTBEAT_SEC=15

# Internal helper input path used by PDF conversion scripts.
# INPUT_PDF_PATH=

# Ingest wrapper mode: `extract`, `ingest`, or `resume`.
INGEST_MODE=ingest

# Error policy for ingest run: `fail` or `skip`.
INGEST_ON_ERROR=skip

# Existing ingest task ID used when `INGEST_MODE=resume`.
# INGEST_TASK_ID=

# Force reprocessing even when hashes/state indicate up-to-date.
# INGEST_FORCE=

# Source PDF directory used by extract mode.
INGEST_PDF_DIR=var/pdfs

# Source chunks directory used by ingest mode.
INGEST_CHUNKS_DIR=var/extracted

# Output directory for extract-only mode chunk files.
INGEST_EXTRACT_OUTPUT_DIR=var/extracted

# Python interpreter path for local host scripts.
PYTHON_BIN=.venv/bin/python

# Embeddings batch size during ingest indexing.
INGEST_EMBED_BATCH_SIZE=128

# Allow unauthenticated API access when set to true.
ALLOW_ANONYMOUS=false

# Optional explicit API key passed to create-api-key helper script.
# API_KEY=

# API key tier assigned by create-api-key helper (`pro`, etc.).
TIER=pro

# Whether generated API key has citations entitlement enabled.
CITATIONS_ENABLED=false

# Enable request prompt logging when truthy.
LOG_PROMPTS=1

# Enable completion text/stream logging when truthy.
LOG_COMPLETIONS=1

# Application log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
LOG_LEVEL=INFO

# Log output format: `pretty` or `json`.
LOG_FORMAT=pretty
