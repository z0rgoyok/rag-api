# Postgres
DATABASE_URL=postgresql://rag:rag@localhost:56473/rag

# Inference provider (OpenAI-compatible). Works with LM Studio or external providers.
# Local (non-docker LM Studio): http://localhost:1234/v1
# Docker Desktop (compose -> host LM Studio): http://host.docker.internal:1234/v1
INFERENCE_BASE_URL=http://localhost:1234/v1
INFERENCE_API_KEY=
INFERENCE_CHAT_MODEL=local-model
INFERENCE_EMBEDDING_MODEL=local-embedding-model

# Optional split providers (override INFERENCE_*):
# CHAT_BASE_URL=
# CHAT_API_KEY=
# CHAT_MODEL=
#
# EMBEDDINGS_BASE_URL=
# EMBEDDINGS_API_KEY=
# EMBEDDINGS_MODEL=
# Embeddings backend:
# - openai_compat: direct OpenAI-compatible HTTP calls (default)
# - litellm: route via LiteLLM (supports many providers)
EMBEDDINGS_BACKEND=openai_compat

# Vertex AI (when using LiteLLM + `EMBEDDINGS_MODEL=vertex_ai/...`):
# EMBEDDINGS_VERTEX_PROJECT=
# EMBEDDINGS_VERTEX_LOCATION=us-central1
# EMBEDDINGS_VERTEX_CREDENTIALS=  # optional path to service account json (ADC also works)

# Legacy (still supported): LMSTUDIO_*
# LMSTUDIO_BASE_URL=http://localhost:1234/v1
# LMSTUDIO_API_KEY=
# LMSTUDIO_CHAT_MODEL=local-model
# LMSTUDIO_EMBEDDING_MODEL=local-embedding-model

# RAG behavior
TOP_K=6
MAX_CONTEXT_CHARS=24000

# Security (MVP)
# Create keys via `python -m apps.api.scripts.create_api_key`
ALLOW_ANONYMOUS=false
