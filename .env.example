# Metadata DB (auth keys + ingest task state)
DATABASE_URL=postgresql://rag:rag@localhost:56473/rag

# Vector DB (semantic index)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=rag_segments

# Inference provider (OpenAI-compatible). Works with LM Studio or external providers.
# Local (non-docker LM Studio): http://localhost:1234/v1
# Docker Desktop (compose -> host LM Studio): http://host.docker.internal:1234/v1
# Keep empty for auto:
# - host scripts -> fallback to http://localhost:1234/v1
# - docker compose api -> fallback to LMSTUDIO_BASE_URL (host.docker.internal by default in compose)
INFERENCE_BASE_URL=
INFERENCE_API_KEY=
INFERENCE_CHAT_MODEL=local-model
INFERENCE_EMBEDDING_MODEL=local-embedding-model

# Optional split providers (override INFERENCE_*):
# CHAT_BASE_URL=
# CHAT_API_KEY=
# CHAT_MODEL=
# CHAT_BACKEND=openai_compat  # or `litellm` (e.g. Gemini)
# Gemini via LiteLLM (example):
# CHAT_BACKEND=litellm
# CHAT_MODEL=gemini/gemini-1.5-flash
# CHAT_API_KEY=  # Gemini API key
# Vertex AI chat via LiteLLM (example):
# CHAT_BACKEND=litellm
# CHAT_MODEL=vertex_ai/gemini-2.0-flash
# CHAT_VERTEX_PROJECT=
# CHAT_VERTEX_LOCATION=us-central1
# CHAT_VERTEX_CREDENTIALS=  # optional path to service account json (ADC also works)
#
EMBEDDINGS_BASE_URL=
# EMBEDDINGS_API_KEY=
# EMBEDDINGS_MODEL=
# Embeddings backend:
# - openai_compat: direct OpenAI-compatible HTTP calls (default)
# - litellm: route via LiteLLM (supports many providers)
EMBEDDINGS_BACKEND=openai_compat

# Vertex AI (when using LiteLLM + `EMBEDDINGS_MODEL=vertex_ai/...`):
# EMBEDDINGS_VERTEX_PROJECT=
# EMBEDDINGS_VERTEX_LOCATION=us-central1
# EMBEDDINGS_VERTEX_CREDENTIALS=  # optional path to service account json (ADC also works)

# Legacy (still supported): LMSTUDIO_*
# LMSTUDIO_BASE_URL=http://localhost:1234/v1
# LMSTUDIO_API_KEY=
# LMSTUDIO_CHAT_MODEL=local-model
# LMSTUDIO_EMBEDDING_MODEL=local-embedding-model

# RAG behavior
TOP_K=6
MAX_CONTEXT_CHARS=24000
RETRIEVAL_USE_FTS=1          # set 0 for pure vector retrieval

# Chunking strategy for ingestion
# - sliding: fixed-size window with overlap (fast, may cut mid-sentence)
# - recursive: hierarchical split by paragraphs/sentences/words (good balance)
# - semantic: split where embedding similarity drops (best for books, requires chonkie)
# - docling_hierarchical: use docling-core structural chunker (host/local extract only)
# - docling_hybrid: use docling-core hybrid chunker (host/local extract only)
CHUNKING_STRATEGY=semantic
CHUNKING_CHUNK_SIZE=512
# CHUNKING_OVERLAP_CHARS=200       # only for sliding strategy
# CHUNKING_SIMILARITY_THRESHOLD=0.5  # only for semantic strategy
CHUNK_SANITIZE_ENABLED=1
CHUNK_SANITIZE_MIN_WORDS=3
CHUNK_SANITIZE_DEDUP=1

# Ingestion: PDF extraction + debug dumps
# IMPORTANT:
# - Docker image intentionally excludes docling/pdf extraction dependencies.
# - `scripts/ingest.sh` is mode-driven and always runs from host:
#   - INGEST_MODE=extract -> `pdf_extract` in CLI
#   - INGEST_MODE=ingest -> `chunks_full` in CLI
#   - INGEST_MODE=resume -> resume existing task by id
# - Backward aliases are accepted: pdf_extract, chunks_full.
# PDF_TEXT_EXTRACTOR=docling
# PDF_DUMP_MD=1                  # write extracted text to `var/extracted/*.md`
# PDF_DUMP_DIR=var/extracted     # must stay under `var/`
# scripts/ingest.sh options:
# INGEST_MODE=ingest             # extract | ingest | resume
# INGEST_ON_ERROR=fail           # or skip
# INGEST_TASK_ID=                # required for INGEST_MODE=resume
# INGEST_PDF_DIR=var/pdfs        # used by INGEST_MODE=extract
# INGEST_CHUNKS_DIR=var/extracted  # used by INGEST_MODE=ingest
# INGEST_EXTRACT_OUTPUT_DIR=var/extracted
# PYTHON_BIN=.venv/bin/python     # local Python interpreter
# INGEST_EMBED_BATCH_SIZE=128     # batch size for embeddings requests during ingest
# DOCLING_OCR_AUTO_TEXT_LAYER_THRESHOLD=0.9

# Security (MVP)
# Create keys via `python -m apps.api.scripts.create_api_key`
ALLOW_ANONYMOUS=false
